{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from src.agent import TextToSQLConverter\n",
    "from src.helpers import load_test_dataset\n",
    "from src.evaluation import SQLQueryComparison, test_agent_on_dataset\n",
    "\n",
    "from src.prompts import zero_shot_cot_prompt, plan_and_solve_cot_prompt, plan_and_solve_cot_prompt_v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "open_ai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=open_ai_api_key)\n",
    "sample_database = \"sqlite:///data/dev_databases/california_schools/california_schools.sqlite\"\n",
    "test_datasets = load_test_dataset(\"data/\")\n",
    "\n",
    "converter_zero_shot_cot = TextToSQLConverter(sample_database, llm, chain_of_thoughts_prompt=zero_shot_cot_prompt)\n",
    "converter_plan_and_solve_cot = TextToSQLConverter(sample_database, llm, chain_of_thoughts_prompt=plan_and_solve_cot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero shot COT prompt end result example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(converter_zero_shot_cot.prompt)\n",
    "\n",
    "sql_query = converter_zero_shot_cot.convert_to_sql(\"How many students are in the school?\")\n",
    "print(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and solve COT prompt and result example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(converter_plan_and_solve_cot.prompt)\n",
    "\n",
    "sql_query = converter_plan_and_solve_cot.convert_to_sql(\"How many students are in the school?\")\n",
    "print(sql_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-2024-08-06\", temperature=0, openai_api_key=open_ai_api_key)\n",
    "evaluation_agent = SQLQueryComparison(llm)\n",
    "test_datasets = test_datasets[:3]\n",
    "results = []\n",
    "\n",
    "agents_definitons = [\n",
    "    (\"\", \"no_cot\"),\n",
    "    (zero_shot_cot_prompt, \"zero_shot_cot\"),\n",
    "    (plan_and_solve_cot_prompt, \"plan_and_solve_cot\"),\n",
    "    (plan_and_solve_cot_prompt_v2, \"plan_and_solve_cot_v2\"),\n",
    "]\n",
    "\n",
    "mlflow.set_experiment(f\"sql_agent_interview_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "for cot_prompt, agent_name in agents_definitons:\n",
    "    results = []\n",
    "    with mlflow.start_run(run_name=agent_name):\n",
    "        for dataset in test_datasets:\n",
    "            evaluations = test_agent_on_dataset(dataset, llm, cot_prompt, agent_name, evaluation_agent, max_questions=15)\n",
    "            results += evaluations\n",
    "\n",
    "        df = pd.DataFrame([i.dict() for i in results])\n",
    "        df[['is_runnable', 'returns_correct_result']] = df[['is_runnable', 'returns_correct_result']].astype(int)\n",
    "        mean_resutls = df[['is_runnable', 'returns_correct_result', 'evaluation_time', 'tokens_used']].mean().to_dict()\n",
    "        mlflow.log_params({'cot_prompt': cot_prompt})\n",
    "        mlflow.log_metric(\"is_runnable_ratio\", mean_resutls['is_runnable'])\n",
    "        mlflow.log_metric(\"correct_results_ratio\", mean_resutls['returns_correct_result'])\n",
    "        mlflow.log_metric(\"average_evaluation_time\", mean_resutls['evaluation_time'])\n",
    "        mlflow.log_metric(\"average_tokens_used\", mean_resutls['tokens_used'])\n",
    "        df.to_csv(f\"{agent_name}_detailed_metrics.csv\", index=False)\n",
    "        mlflow.log_artifact(f\"{agent_name}_detailed_metrics.csv\")\n",
    "        os.remove(f\"{agent_name}_detailed_metrics.csv\")\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
